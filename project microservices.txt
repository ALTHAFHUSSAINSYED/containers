1️⃣ Architecture Overview

EMart is a microservices-based e-commerce application.

Front-end: Angular client app accessed via an Nginx API Gateway.

Nginx listens on endpoints:

/ → Angular front-end

/api → NodeJS “Mart API” backend

/webapi → Java “Books API” backend

Databases:

MongoDB → used by NodeJS API

MySQL → used by Java Books API

Each service (Angular, NodeJS, Java, MongoDB, MySQL, Nginx) is deployed in its own container.

Takeaway: Each microservice is independent, isolated, and communicates through APIs.

2️⃣ Docker & Docker Compose

Docker Compose is used to:

Build images from Dockerfiles (build:)

Or pull images directly from Docker Hub (image:)

Define networks, volumes, environment variables, and port mappings

Automatically restart containers if they crash (restart: always can be added)

Command flow:

docker compose build → builds images from Dockerfiles

docker compose up -d → runs containers in detached mode

docker compose ps → check running containers

docker compose down → stop and remove containers

Tip: This setup makes deployment repeatable and isolated, perfect for microservices.

3️⃣ Service Communication

Microservices communicate via API calls, not direct database access.

Nginx routes requests to NodeJS (/api) and Java (/webapi) APIs.

NodeJS uses MongoDB via internal Docker network service name (mongodb:27017)

Java Books API uses MySQL via internal Docker network service name (mysql:3306)

Takeaway: Docker service names act like DNS inside the network; containers don’t need IP addresses.

4️⃣ Volumes and Persistence

Databases have volumes to persist data:

vprodbdata:/var/lib/mysql (MySQL)

vproappdata:/usr/local/tomcat/webapps (Java app)

Volumes ensure that data survives container restarts.

5️⃣ Running the App

Access front-end via VM IP (e.g., http://10.0.2.15/)

Register a user → data goes to MySQL or MongoDB depending on service

Login → API validates user → Angular shows product list

Important points for DevOps:

Docker internal networks handle service-to-service communication.

No need to expose DB ports to the host unless for testing.

6️⃣ Cleanup

docker compose down → stop & remove containers

docker system prune -a → remove unused images & networks

vagrant halt → stop VM

Why: Keeps VM clean, avoids conflicts, and frees resources.

✅ Key DevOps Takeaways

Isolation: Each microservice runs in its container, independent of others.

Scalability: Microservices can scale individually (add more NodeJS or Java containers if traffic increases).

Portability: Docker images ensure app works the same across environments.

API-driven: Front-end and services communicate via APIs, not direct DB queries.

Repeatable deployments: Docker Compose file acts as infrastructure as code.

how Service Communication happens explain in detail?

1️⃣ Basic Concept

In a microservices architecture:

Each service is independent and runs in its own container.

Services don’t directly access each other’s internal data (like databases).

Instead, services communicate via APIs (HTTP/REST, gRPC, or message queues like RabbitMQ).

Think of services like separate apps talking through defined doors (APIs).

2️⃣ Docker Networking & Service Names

When you run services with Docker Compose, it creates a default network.

Key points:

Containers in the same Docker Compose file are automatically on the same network.

Each container can be reached via its service name (defined in docker-compose.yml) as a hostname.

Example from EMart:

services:
  vprodb:      # MySQL container
  vproapp:     # Java application
  vproweb:     # Frontend Angular app


Inside the Java container (vproapp):

To connect to MySQL, it can use hostname vprodb and port 3306, like this:

jdbc:mysql://vprodb:3306/emartdb


No need to use IPs. Docker handles the service name → container IP mapping internally.

3️⃣ HTTP API Communication

Most microservices communicate over HTTP/REST APIs.

For EMart:

User → Frontend:

User types http://<VM-IP>/ → Nginx routes request to Angular frontend.

Frontend → NodeJS API (/api endpoint):

Angular makes API calls via HTTP:

POST /api/register
GET /api/products


NodeJS API → MongoDB:

NodeJS uses internal MongoDB hostname (mongodb) to read/write data.

Frontend → Java API (/webapi endpoint):

Angular calls http://nginx/webapi/books

Java API → MySQL:

Java uses hostname emartdb (MySQL container) to fetch/store data.

Key idea:

Each service only knows how to call the other service’s API.

No service directly depends on another service’s internal logic or database.

4️⃣ Messaging Queues (Optional)

Sometimes, services need asynchronous communication:

Example: When an order is placed, NodeJS service might notify Java service about payment status.

Instead of HTTP, it can use RabbitMQ:

NodeJS publishes a message to a queue.

Java consumes the message and processes it.

This decouples services, improving reliability and scalability.

5️⃣ Nginx API Gateway Role

Nginx acts as a reverse proxy and API gateway:

Receives user requests.

Routes requests to the correct backend service:

/api → NodeJS

/webapi → Java

Ensures single entry point for the user.

Simplifies cross-service communication by exposing only necessary endpoints.

6️⃣ Visual Summary
[User Browser]
       |
       v
   [Nginx API Gateway]
     /           \
 /api            /webapi
 |                 |
[NodeJS API]     [Java API]
   |                 |
[MongoDB]         [MySQL]


Communication inside the Docker network uses service names.

Communication outside (user → frontend) uses VM IP / Nginx.

✅ Takeaways

Services are isolated, communicate via APIs or message queues.

Docker service names make internal networking easy and dynamic.

API Gateway simplifies routing and hides internal service details.

Asynchronous messaging improves decoupling and reliability.


Communication inside the Docker network uses service names. Communication outside (user → frontend) uses VM IP / Nginx. then where API is actually using for communication then ?

1️⃣ Inside the Docker network (container → container)

Each microservice runs in its own container.

They cannot use localhost to talk to other containers.

Instead, they use API calls (HTTP/REST, gRPC) to each other.

Docker Compose gives each container a hostname = service name.

Example:

services:
  nodeapi:
    image: nodeapp
  mysql:
    image: mysql


Inside nodeapi container, to connect to MySQL:

mysql -h mysql -u root -p


Here mysql is the service name, resolved by Docker’s internal DNS.

API in this case could be SQL commands or HTTP calls — it depends on the service.

Rule: Whenever one service wants data or functionality from another service, it does so via API endpoints exposed by that service.

2️⃣ Outside the Docker network (user → frontend)

The user doesn’t access containers directly; they go through API Gateway (Nginx).

User types in http://<VM-IP>/.

Nginx forwards requests to the correct container based on URL paths:

/api     -> NodeJS container
/webapi  -> Java container


The API exposed here is HTTP. For example:

POST http://<VM-IP>/api/register


The frontend Angular app calls these APIs internally to get data.

3️⃣ Where APIs are actually used

APIs are the “contract” between services.

Communication paths:

Frontend → Backend services

Angular calls NodeJS via /api → HTTP requests.

Backend service → Backend service

NodeJS talks to MongoDB → Database API (native protocol).

NodeJS could also call Java service via /webapi → HTTP.

Asynchronous messaging

NodeJS publishes message → RabbitMQ → Java consumes it.

So APIs exist both inside Docker network (service-to-service) and outside (user-to-frontend).

Inside: service name + port is used.

Outside: VM IP + port is used.

✅ Key point:

API = the way services talk to each other.

How the network resolves the target differs:

Internal: Docker service name.

External: VM IP / Nginx port.

what are these / /api /webapi ? are they folders in containers ?

They aren't folders inside the containers. Think of them more like signposts or addresses that Nginx uses to figure out where to send incoming traffic. This process is called routing.

Let's use an analogy. Imagine Nginx is a receptionist in a big office building.

When a visitor arrives at the main entrance (your server's IP address), they tell the receptionist where they want to go.

The /, /api, and /webapi paths are like telling the receptionist, "I'm here to see the front-end team," or "I have a meeting with the Mart API team."

Based on that path, the Nginx "receptionist" directs the request to the correct "office" (the correct Docker container).

Let's break down each one:
1. / (The Root Path)
What it is: This is the default path. When you just type http://<your-vm-ip>/ into your browser, Nginx sees the / path.

Routing Rule: The configuration tells Nginx, "Any request for the root path should go to the Angular front-end container." 
This is how the main webpage loads.

2. /api (The Mart API Endpoint)
What it is: This is a specific path designated for the NodeJS service.

Routing Rule: When the Angular app makes a request to http://<your-vm-ip>/api/products, Nginx sees the /api part and knows, "Okay, this request is for the NodeJS 'Mart API' backend." It then forwards the request to that specific container.

3. /webapi (The Books API Endpoint)
What it is: This path is reserved for the Java service.

Routing Rule: If the Angular app needs to get a list of books and sends a request to http://<your-vm-ip>/webapi/books, Nginx sees /webapi and routes it directly to the Java 'Books API' container.

So, these are virtual paths defined in your Nginx configuration. They exist to organize traffic, not as actual folders on a disk.

Based on that, if your Angular front-end needed to get user registration data, which is handled by the NodeJS service, what would the beginning of its request URL look like?


ok, where this routing table located in container default network or in application code ?

The routing rules aren't part of the Docker network itself, nor are they in the application code of the backend services (NodeJS/Java).

They are located in the Nginx container's configuration file.

Think of it this way:

The Application Code (Angular, NodeJS) decides what call to make. For example, the Angular app knows it needs to fetch data by calling /api/products.

The Docker Network is like the road system that connects all the buildings (containers). It ensures a path exists from Nginx to the NodeJS container.

The Nginx Configuration File is the map or the set of instructions used by the "receptionist" (Nginx). This file explicitly tells Nginx what to do with requests based on their path.

A simplified version of that Nginx configuration file (nginx.conf) would look something like this:

Nginx

server {
    listen 80;

    # Rule 1: Default traffic
    location / {
        # Send it to the Angular front-end service
        proxy_pass http://vproweb:80;
    }

    # Rule 2: Traffic for the NodeJS API
    location /api {
        # Send it to the NodeJS API service
        proxy_pass http://vproapp:8080; # Assuming NodeJS app runs on this service name/port
    }

    # Rule 3: Traffic for the Java API
    location /webapi {
        # Send it to the Java API service
        proxy_pass http://vprobook:8080; # Assuming Java app runs on this service name/port
    }
}
Each location block is a rule that matches a URL path and tells Nginx where to forward (proxy_pass) the request.

how can see and access Nginx container's configuration file from a container cli ?

Here’s the step-by-step process:

1. Get a Shell Inside the Nginx Container
First, you need to open an interactive command line (a shell) inside the running Nginx container. The command for this is docker compose exec.

Let's assume your Nginx service is named vproweb in your docker-compose.yml file.

Bash

# Command to execute a shell inside the 'vproweb' container
docker compose exec vproweb sh
What this command does:

docker compose exec: Tells Docker Compose to execute a command in a running service.

vproweb: The name of the service (container) you want to enter.

sh: The command you want to run, in this case, the basic shell program. (Some containers have bash, but sh is more common in minimal images).

After running this, your terminal prompt will change, indicating you are now inside the Nginx container.

2. Locate and View the Configuration File
Once you're inside the container, you can navigate its file system. Nginx configuration files are almost always located in the /etc/nginx/ directory.

The main configuration file is usually /etc/nginx/nginx.conf. However, the specific routing rules are often in separate files inside the /etc/nginx/conf.d/ directory.

You can view the contents of these files using commands like cat, less, or more.

Bash

# Inside the container shell:

# List all files in the main config directory
ls /etc/nginx/

# View the main configuration file
cat /etc/nginx/nginx.conf

# List the specific routing rule files
ls /etc/nginx/conf.d/

# View the specific configuration file for your application (e.g., default.conf)
cat /etc/nginx/conf.d/default.conf
The last command, cat /etc/nginx/conf.d/default.conf, is most likely where you'll find the location /, location /api, and location /webapi routing blocks.

3. Exit the Container
When you're finished, simply type exit and press Enter to leave the container's shell and return to your own terminal.

Bash

# Inside the container shell:
exit
💡 Important DevOps Note: While you can use tools like vi or nano to edit files inside a container (if they're installed), it's strongly discouraged. Any changes you make will be lost if the container is ever recreated. The correct way to change the configuration is to edit the source nginx.conf file on your host machine and then rebuild and restart your services with docker compose up --build -d.